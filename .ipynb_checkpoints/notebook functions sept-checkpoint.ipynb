{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import timeit\n",
    "from lxml import html\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "processed_match=pd.Series()\n",
    "\n",
    "def extract_commentary_ESPN(id):\n",
    "    id=str(id)\n",
    "    #try para evitar que pete el programa si no puede cargar bien la url\n",
    "    try:\n",
    "        page = requests.get('http://www.espnfc.com/commentary/'+id+'/commentary.html')\n",
    "        tree = html.fromstring(page.text)\n",
    "        times = tree.xpath('//div[@class=\"timestamp\"]/p/text()')\n",
    "        comments = tree.xpath('//div[@class=\"comment\"]/p/text()')\n",
    "        try:\n",
    "            df = pd.DataFrame({'times':times, 'comments':comments})\n",
    "        except:\n",
    "            df=pd.DataFrame()\n",
    "        return df\n",
    "    #si hay problemas para cargar la página, devolverá un dataframe vacío\n",
    "    except:\n",
    "        df=pd.DataFrame()\n",
    "        return df\n",
    "\n",
    "#remove the last two characters of \"times\" column\n",
    "def delete_last(a):\n",
    "    return (a[:-1])\n",
    "\n",
    "#calculate minute of first corner\n",
    "def calculate_minc1(df):\n",
    "    #remove the last two characters of \"times\" column\n",
    "    df.times=df.times.apply(delete_last)\n",
    "    #convert times to number, since they are a string\n",
    "    df.times=df.times.convert_objects(convert_numeric=True)\n",
    "    #filtering rows where comment string starts with 'Corner' and returning minimum time\n",
    "    return np.amin(df[df.comments.str.startswith('Corner')]).times\n",
    "\n",
    "#Extract Match Statistics: teams, total corners\n",
    "def extract_statistics_ESPN(id):\n",
    "    id=str(id)\n",
    "    page = requests.get('http://www.espnfc.com/gamecast/statistics/id/'+id+'/statistics.html')\n",
    "    tree = html.fromstring(page.text)\n",
    "    team_away = tree.xpath('//div[@class=\"team away\"]/p/a/text()')#visitante\n",
    "    team_home=tree.xpath('//div[@class=\"team home\"]/p/a/text()')#local\n",
    "    script=tree.xpath('//*[@id=\"matchcenter-'+id+'\"]/div[1]/p[1]/span/script/text()')\n",
    "    try:\n",
    "        timestamp = float(script[0][52:65])\n",
    "        fecha=time.strftime(\"%a %d %b %Y %H:%M:%S GMT\", time.gmtime(timestamp / 1000.0))\n",
    "    except:\n",
    "        fecha=\"UNKNOWN\"\n",
    "    competicion = tree.xpath('//div[@class=\"match-details\"]/p[@class=\"floatleft\"]/text()')\n",
    "    try:\n",
    "        competicion= competicion[0].strip()\n",
    "    except:\n",
    "        competicion=\"ERR COMPETICION\"\n",
    "    home_corners = tree.xpath('//td[@id=\"home-corner-kicks\"]/text()')\n",
    "    away_corners = tree.xpath('//td[@id=\"away-corner-kicks\"]/text()')\n",
    "    return pd.Series([id,team_away,team_home,competicion,home_corners,away_corners,fecha],index=['id','team_home','team_away','competition','corners_home','corners_away','date'])\n",
    "\n",
    "#Put together the extracted info\n",
    "def process_match(id):\n",
    "    global processed_match\n",
    "    global bad_match\n",
    "    my_match=extract_commentary_ESPN(id)\n",
    "    if len(my_match)>1:\n",
    "        #en caso de que haya problemas de cargar url, que no pete\n",
    "        try:\n",
    "            processed_match=extract_statistics_ESPN(id)\n",
    "            minc1=pd.Series([calculate_minc1(my_match)],index=['minc1'])\n",
    "            processed_match=processed_match.append(minc1)\n",
    "            bad_match=False\n",
    "        except:\n",
    "            #si no ha podido cargar la url, lo indica como partido inválido\n",
    "            bad_match=True\n",
    "    else:\n",
    "        bad_match=True\n",
    "    return processed_match,bad_match\n",
    "\n",
    "#We need to extract chunks of matches and present them in a dataframe\n",
    "def analyze_chunk(ids):\n",
    "    partidos_df=pd.DataFrame()\n",
    "    for id in ids:\n",
    "        match_stats,partido_incorrecto=process_match(id)\n",
    "        if partido_incorrecto==False:\n",
    "            partidos_df=partidos_df.append(match_stats,ignore_index=True)\n",
    "    return partidos_df\n",
    "\n",
    "#Append results to a csv file\n",
    "def append_to_csv(file,df):\n",
    "    f = open(file, 'a') # Añadir los resultados al archivo de corners\n",
    "    df.to_csv(f,header=False,encoding=\"utf-8\")\n",
    "    f.close()\n",
    "\n",
    "#append only new matches\n",
    "def select_matches_append(obtained_df,file):\n",
    "    destination_df=pd.read_csv(file) #read the file\n",
    "    obtained_df.id=obtained_df.id.convert_objects(convert_numeric=True)\n",
    "    criterion=obtained_df.id.isin(destination_df.id)\n",
    "    append_to_csv(file,obtained_df[-criterion])#inverse, because we want to include what is NOT already in the file\n",
    "\n",
    "#We will save time by not extracting info from already-scraped matches\n",
    "def select_ids(id_arange,file):\n",
    "    destination_df=pd.read_csv(file) #read the file\n",
    "    criterion=id_arange.isin(destination_df.id)\n",
    "    return id_arange[-criterion].values\n",
    "\n",
    "#study and save a range\n",
    "def study_range (id_ini,id_fin,corner_file):\n",
    "    #check what id_matches from the range have already been scraped\n",
    "    raw_range=pd.Series(np.arange(id_ini,id_fin+1))\n",
    "    analysis_range=select_ids(raw_range,corner_file)\n",
    "    #perform scraping of missing id_matches\n",
    "    chunk_df=analyze_chunk(analysis_range)\n",
    "    #append results to file if there are any\n",
    "    try:\n",
    "        #select_matches_append(chunk_df,corner_file)\n",
    "        append_to_csv(corner_file,chunk_df)\n",
    "    except:\n",
    "        print \"NOTHING ADDED\"\n",
    "        \n",
    "#study in chunks and calculate time    \n",
    "def study_in_chunks(ini,last,step,file):\n",
    "    for i in range(ini,last+1,step):\n",
    "        start = timeit.default_timer()\n",
    "        study_range(i,i+step,file)\n",
    "        print i\n",
    "        stop = timeit.default_timer()\n",
    "        print (stop - start)/60\n",
    "        print \"MINUTOS\"\n",
    "\n",
    "def get_ids(date_number):\n",
    "    from lxml import html\n",
    "    import requests\n",
    "    date_number=str(date_number)\n",
    "    page = requests.get('http://www.espnfc.com/scores?date='+date_number)\n",
    "    tree = html.fromstring(page.text)\n",
    "    return tree.xpath('//div[@class=\"score full\"]/@data-gameid')\n",
    "\n",
    "def study_date(date,corner_file):\n",
    "    ids2=get_ids(date)\n",
    "    ids=pd.Series(ids2)\n",
    "    analysis_range=select_ids(ids,corner_file)\n",
    "    chunk_df=analyze_chunk(analysis_range)\n",
    "    #append results to file if there are any\n",
    "    try:\n",
    "        #select_matches_append(chunk_df,corner_file)\n",
    "        append_to_csv(corner_file,chunk_df)\n",
    "        print \"STUDIED\"\n",
    "        print date\n",
    "        print \"Added\"\n",
    "        print len(chunk_df)\n",
    "    except:\n",
    "        print \"NOTHING ADDED\"\n",
    "\n",
    "\n",
    "def several_dates(datelist,corner_file):\n",
    "    for date in datelist:\n",
    "        study_date(date,corner_file)\n",
    "\n",
    "#para hacerlo a saco    \n",
    "#process_match(397351)\n",
    "#study_in_chunks(397351,419900,50,'corners_append.csv')\n",
    "#study_in_chunks(426800,434300,50,'corners_append.csv')\n",
    "#study_in_chunks(424200,424800,50,'corners_append.csv')\n",
    "#study_in_chunks(393300,393800,50,'corners_append.csv')\n",
    "#para buscar los ids en una fecha y extraer ESOS\n",
    "study_date(20150912,'corners_append.csv')\n",
    "#study_date(20150829,'corners_append.csv')\n",
    "#study_date(20150830,'corners_append.csv')\n",
    "#study_date(20150831,'corners_append.csv')\n",
    "\n",
    "#para hacerlo con muchas fechas\n",
    "#datelist=(20150821,20150822,20150823,20150824)\n",
    "#several_dates(datelist,'corners_append.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
